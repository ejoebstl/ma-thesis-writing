\section{Acoustic Modelling using Neural Networks}

When we use neural networks for acoustic modelling, we usually use the neural network as a part of the acoustic model. As mentioned in section \ref{sec:acoustic_model}, acoustic models combine a discriminative classification algorithm with a hidden markov model. Neural networks can be used as such a discriminative classification algorithm. A neural network can be used to directly predict the likelihood of a state $s_i$ of the hidden markov model $p(x|s_i)$ given a feature $x$. This makes our model essentially a markov chain, since the states are now assumed to be directly observable. In \ref{hinton2012deep}, an excellent summary of the approach is given, although first experiments with neural network based acoustic modeling can be found in the early 90s \cite{bengio1993connectionist}. Notably, recent advancements were with robust acoustic modeling using TDNNs \cite{peddinti2015jhu}. \\ \\
We can formalize this approach in the framework of hidden markov models by defining exactly a single symbol $v_i$ for each state $s_i$, with the following emission probabilities:
\[
b_j(k) = \delta_{bk}
\]
Where $\delta_{ij}$ is the so called \textit{kronecker delta}.
\[
\delta_{ij} = \begin{cases}
1 & i = j\\
0 & \text{otherwise}
\end{cases} 
\]
This formalization might seem unnecessary, but eases the transition from hidden markov model training to neural network training for certain training algorithms. In this case, our neural network is used to predict the likelihood $p(x|b_i)$ of a certain symbol $b_i$.\\
We will now introduce two loss functions that are used in the field of automated speech recognition. We will also show how we can form gradients from the given loss functions. The gradients can be used to train neural networks according to the back-propagation algorithm described in section \ref{sec:gradient_descend}. Brief versions of this derivations can be found in \cite{ghoshal2013sequence}, detailed analysis of the loss functions can be found in \cite{gibson2008minimum} and \cite{povey2005discriminative}.
\subsection{Maximum Likelihood}
For maximum likelihood training, we use the viterbi algorithm to calculate the most likely state sequence. Then, each audio frame is labeled with its most likely state, according to the viterbi pass. We treat each state as a separate class and use this dataset for training a model using the negative log posterior as loss function \cite{kitasr2018stueker}. Formally, this loss function be written for an utterance as:
\[
\mathcal{L}_{\text{ML}} = - \sum_{t = 0}^{n} \log p^*_{s_t}
\]
Where $n$ is the length of the utterance, $s_t$ is the correct state at time $t$, given by the viterbi pass, $\theta$ are the model parameters and $o_t$ is the observed feature vector at time $t$. $p^*_{s_t}$ a shorthand for the predicted posterior probability for state $s_t$ produced by our model, formally $p^*(s_t|\theta,o_t)$. For a single frame, this loss function is equal to the negative log likelihood loss from equation \ref{eq:nlll}. \\ \\
We now assume that the probability $p^*_{s_t}$ is produced by the output of a neural network model, which uses a softmax activation as the final layer. Let $y^*_{s_j}$ be the output of the layer before the softmax layer for state $s_j$. We can calculate the gradient for our loss function for a single frame with respect to $y^*_{s_j}$ as follows:
\begin{align*}
\frac{\partial\mathcal{L}_\text{ML}}{\partial y^*_{s_j}} &= -\frac{\partial \log p^*_{s_t}}{\partial y^*_{s_j}} \\
&= -\frac{\partial \; \log \frac{\exp \left(y^*_{s_t}\right)}{\exp\left(\sum_{i = 0}^{n} y^*_{s_j}\right)}}{\partial y^*_{s_j}} \\
&= -\frac{\partial \; y^*_{s_t} - \sum_{i = 0}^{n} y^*_{s_j}}{\partial y^*_j} \\
&= 1 - \delta_{tj}
\end{align*}
Where $\delta_{ij}$ is the kronecker delta. \\ \\
After the neural network acoustic model was trained over the whole training set, labels can be re-written, and another neural network acoustic model can be trained with the new labels. This process can be iterated several times to improve results. 
\subsection{Maximum Mutual Information Estimation}
\textit{Maximum mutual information} (\textit{MMI}) estimation was introduced for estimating hidden markov model parameters in speech recognition systems in \ref{bahl1986maximum}. The training criterion given in \ref{bahl1986maximum} maximizes the ability of the model to discriminate between the correct distribution and any other distribution. Let $\mathcal{V}$ be the set of all utterances. In the context of speech recognition, we can give a loss function that maximizes mutual information between an observation sequence $O_U = (o_{U,1}, ..., o_{U,n})$ for a word sequence $U \in V$ as follows:
\[
\mathcal{L}_{\text{MMI}} = -\log\frac{p(O_U|U)P(U)}{\sum_{V \in \mathcal{V}} p(O_U|V)P(V)} 
\]
In \cite{ghoshal2013sequence}, a very similar formulation is given, which is maximized for all utterances, while our loss function is minimized for a single utterance, which is more convenient when working with neural networks. The original formulation af a convenient optimization criterion for MMI estimation was given in \cite{schluter1998comparison}.\\
Since we are working with automated speech recognition, we can assume that our word sequences can be separated to state sequences $S_U = (s_{U,1},...,s_{U,n})$ which we found using our speech recognition system with $P(U) = \prod_{t = 0}^{n} p^*(s_{U,t})$ and similar for $O_U$. This approach was also chosen in \ref{bahl1986maximum}, to simplify the error criterion. Furthermore, we replace the set $\mathcal{V}$ by the set $\mathcal{M}$, which contains the $m$ best state sequences found during our forward-backward pass for the utterance $U$, a practical simplification which is given in \cite{schluter1998comparison}.
\[
\mathcal{L}_{\text{MMI}} = -\log\frac{\prod_{t = 0}^{n} p^*(o_{U,t}|s_{U,t})p^*(s_{U,t})}{\sum_{V \in \mathcal{M}} \prod_{t = 0}^{n} p^*(o_{U,t}|s_{V,t})p^*(s_{V,t})} 
\]
With the theorem of bayes, we can expand:
\[
p^*(o_{U,t}|s_{U,t}) = \frac{p^*(s_{U,t}|o_{V,t})}{p^*(s_{U,t})}
\]
With this expansion, we can simplify and express $\mathcal{L}_{\text{MMI}}$ in terms of $p^*(s_{U,t}|o_{U,t})$.
\[
\mathcal{L}_{\text{MMI}} = -\log\frac{\prod_{t = 0}^{n} p^*(s_{U,t}|o_{U,t})}{\sum_{V \in \mathcal{M}} \prod_{t = 0}^{n} p^*(s_{V,t}|o_{U,t})} 
\]
This expression can be derived by the posterior probability $p^*(s_{U,t}|o_{U,t})$ given by our neural network model to calculate a gradient for backpropagation. Again, let $p^*_{s_{j,t}}$ be $p^*(s_{j,t}|o_j)$.
\begin{align*}
\frac{\partial\mathcal{L}_{\text{MMI}}}{\partial p^*_{s_{j,t}}} &= \frac{\partial \log \sum_{V \in \mathcal{M}} \prod_{t = 0}^{n} p^*_{s_{V,t}}}{\partial p^*_{s_{j,t}}} - \sum_{t = 0}^{n} \frac{\partial \log p^*_{s_{U,t}}}{\partial p^*_{s_{j,t}}} \\
&= \frac{ \sum_{V \in \mathcal{M}} \delta_{(s_{V,t}),(s_{j,t})} \prod_{t = 0}^{n} p^*_{s_{V,t}}}{\sum_{V \in \mathcal{M}} \prod_{t = 0}^{n} p^*_{s_{V,t}}} - \frac{1}{p^*_{s_{j,t}}}\\
&= \frac{ \sum_{V \in \mathcal{M}} \delta_{(s_{V,t}),(s_{j,t})} }{|\mathcal{M}|} - \frac{1}{p^*_{s_{j,t}}}
\end{align*}
If we generate the most likely state hypothesis for utterance $U$ with the viterbi algorithm, we can assume all $p^*_{s_{U,t}}$ to be equal to one. This second fraction becomes:
\begin{align*}
\frac{\partial\mathcal{L}_{\text{MMI}}}{\partial p^*_{s_{j,t}}} &= \frac{ \sum_{V \in \mathcal{M}} \delta_{(s_{V,t}),(s_{j,t})} }{|\mathcal{M}|} - \delta_{(s_{i,t}),(s_{U,t})}
\end{align*}
Recall that $p^*_{s_{j,t}}$ is a shorthand for $p^*(s_{j,t}|o_j)$. We can now rewrite the first fraction in terms of probabilities, more precisely the probability of visiting state $s_{j,t}$ while we observe $O_U$. The fraction is indeed this probability: We divide the count of state sequences which visit $s_{j,t}$ by the count of all sequences:
\begin{align*}
\frac{\partial\mathcal{L}_{\text{MMI}}}{\partial p^*_{s_{j,t}}} &= p(x_t = s_{j,t}|O) - \delta_{(s_{i,t}),(s_{U,t})}
\end{align*}
This formulation is familiar. It corresponds to the definition of $\gamma_t(j)$ from section \ref{sec:learning_hmm}, that is produced by the forward-backward algorithm when training hidden markov model parameters. We conclude this derivation by a compact formulation of the gradient for the MMI loss function:
\begin{align}
\label{eq:mmi_grad}
\frac{\partial\mathcal{L}_{\text{MMI}}}{\partial p^*_{s_{j,t}}} &= \gamma_t(j) - \delta_{ij}
\end{align}
Where $i$ denotes the index of the correct state at time $t$, given by a viterbi pass. 
