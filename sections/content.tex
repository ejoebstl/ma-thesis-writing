%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.2, 2017-08-01

% TODO: Dataset for eval: /project/iwslt2015/EN/dbase/tst2014/score?

\chapter{Design of a TDNN acoustic model}
To find a good design for a TDNN acoustics model, we had to make several design decisions. Most of the design decisions were justified by experiments, while some were taken from related work. This chapter summarizes the decisions made and the corresponding results. In some cases, we will also provide a comparison with a fully-connected network, which served as our experiment baseline. It should be noted that the experiments described in \cite{peddinti2015jhu} and \cite{peddinti2015reverberation} provided the main inspiration for this work, therefore we based some of our parameters on their result. \\ 
\section{Training Data and System Setup}
All experiments in this work only concern to the neural network part of the acoustic model, which is a HMM/TDNN hybrid. The speech recognition system itself, as well as all data, the HMM part of the acoustic model, the dictionary and the language model, are taken from the system described in \cite{nguyen20162016}. The system is built upon the Janus recognition toolkit \cite{finke1997karlsruhe}. We utilize a four-gram language model and the CMU Pronouncing Dictionary \cite{cmudict}, which contains 39 phones. The acoustic model has 8156 different distributions, which means that the HMM part of our acoustic model has 8156 different states. \\ \\
The training and test dataset for the acoustic model consisted of 468 hours of english speech from the TED-LIUM v2 \cite{rousseau2014enhancing}, Broadcast News \cite{graff19971996} and Quaero 2010-2012 Datasets. From these 468 hours, 17 hours are randomly selected as test set, 451 hours are used for the training set. \\ \\
The development dataset, used for tuning the decoder parameters, consists of the english IWSLT 2013 evaluation dataset for the ASR track \cite{cettolo2013report}. This dataset consists of 3.9 hours of TEDx talks. 
\section{Neural Network Parameters}
This section focuses on parameters that are related to the neural network design. For all models in this section, the word error rate was estimated by using $l_p$ and $l_z$ that were tuned for each model separately. The priors were estimated by counting labels over the whole training set.
\subsection{Input Features and Input Context}
The time input context of all our TDNN models is ${-13,9}$, which means the TDNN sees the current frame, thirteen frames in the past, and nine frames in the future. This parameter was taken from the smallest TDNN model described in \cite{peddinti2015reverberation}.
Each time frame consists of 40 lMEL features. 
\subsection{Count and Width of Layers}
The count of layers and width of each layer is one of the most important design parameters for neural networks. As in \cite{peddinti2015reverberation}, all our models use the same amount of channels for each TDNN unit. Only the count of observed time frames changes with each layer. \\ \\
\begin{minipage}{\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ylabel=Word Error Rate, xlabel=Channels, height=6cm, 
	xticklabel style={name=T\ticknum},grid=major]
	\addplot coordinates {
		(100,19.4) %994
		(200,17.3) %999
		(300,16.6) %1009
		(400,16.8) %1010
	};
	\addlegendentry{4 Layer};
	\addplot coordinates {
		(40,17.6) %1040
		(100,17.0) %i3
		(200,17.0) %1039
		(300,17.0) %1036
	};
	\addlegendentry{5 Layer};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Word Error Rate for different choices of layer and channel count}
	\label{fig:tdnn_layer_design}
\end{minipage}
Figure \ref{fig:tdnn_layer_design} shows the word error rate for different configurations. Table \ref{tbl:tdnn_layer_design} gives the kernel size and stride over time for each layer in each architecture. \\ \\
\begin{minipage}{\linewidth}
	\centering 
	\begin{tabular} {|l | c | c | c | c | c | c | c | c | c | c | c | c |}
	\hline
	Model & \multicolumn{4}{c|}{4 Layer} & \multicolumn{5}{c|}{5 Layer} \\
	\hline
	Layer & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4 & 5 \\
	\hline
	Kernel Size & 5 & 2 & 2 & 2 & 5 & 5 & 3 & 2 & 2 \\
	\hline
	Stride & 3 & 2 & 2 & 2 & 2 & 1 & 1 & 1 & 1 \\
	\hline
	\end{tabular}
	\captionof{figure}{Splice and stride parameters for the two different architectures}
	\label{tbl:tdnn_layer_design}
\end{minipage}
For this set of experiments, each TDNN layer was followed by a LP-pooling nonlinearity with P of two and group size ten, as well as a batch normalization layer.
\subsection{Nonlinearity}
Following\cite{zhang2014improving}, we use a LP-pooling nonlinearity with a P of two and a group size if ten after each TDNN layer. Using the LP norm can be problematic, as the gradient can become infinity when all inputs in the pooling group are zero. The authors of \cite{peddinti2015reverberation} propose to use a normalization layer after each LP-pooling layer, which solves the problem. We propose an alternate approach, which is setting the gradient to zero if all inputs become zero. Furthermore, we also tested max pooling as a possible nonlinearity. All experiments were conducted on the 4-Layer architecture described before. Figure \ref{fig:tdnn_nonlinearity} shows the results. \\ \\
\begin{minipage}{\linewidth}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
		xbar,xmajorgrids=true,
		width=0.8\linewidth,height=4.5cm, enlarge y limits=0.5,
		xmin=15,xlabel={Word Error Rate},
		symbolic y coords={LP/BatchNorm,MaxPool,LP/GradZero},
		ytick=data,nodes near coords, nodes near coords align={horizontal},
		]
		%1009, I24, I12
		% All with only l[/lz/mb tuning, no custom priors and no softmax
		\addplot coordinates {(15.5,LP/GradZero) (15.8,MaxPool) 
		(16.6,LP/BatchNorm)};
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Word Error Rate for different choices of nonlinearities}
	\label{fig:tdnn_nonlinearity}
\end{minipage} \\ \\
In our case, the usage of LP tuning with a modified gradient outperformed the other nonlinearities.  
\section{Training Parameters}
This section is focused on the training setup for our acoustic model. Out setup is closely related to the setup described in \cite{nguyen20162016}. We also build upon the same dataset which is described in detail in chapter \ref{} TODO link eval. 
\subsection{Shuffling of Dataset}
For our experiments, we benchmarked different shuffling strategies. Shuffling of the whole dataset once before training, and shuffling of the whole dataset before each epoch. The results can be seen in figure \ref{}.
TODO TODO TODO, re-train two feed-forward models with 32b full shuffling and 16b full shuffling \\ \\
\begin{minipage}{\linewidth}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	xbar,xmajorgrids=true,
	width=0.8\linewidth,height=4.5cm, enlarge y limits=0.5,
	xmin=15,xlabel={Word Error Rate},
	symbolic y coords={LP/BatchNorm,MaxPool,LP/GradZero},
	ytick=data,nodes near coords, nodes near coords align={horizontal},
	]
	% TODO TODO TODO 
	% All with only l[/lz/mb tuning, no custom priors and no softmax
	\addplot coordinates {(15.5,LP/GradZero) (15.8,MaxPool) 
		(16.6,LP/BatchNorm)};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Word Error Rate for different shuffling strategy}
	\label{fig:tdnn_shuffling}
\end{minipage} \\ \\
\subsection{Learning Rate and Learning Rate Decay}
As in \cite{nguyen20162016}, we utilize the newbob learning rate scheduler for training for stochastic gradient descend, with an initial learning rate of $0.08$ and a momentum of $5$. Figure \ref{fig:newbob_wer} show the word error rate per epoch when using newbob. The model used for generating this plot was a small TDNN with only three layers we used for testing our implementation. \\ \\
TODO: New graphic for I12 is currently being generated. \\ \\
\begin{minipage}{\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ylabel=Word Error Rate, xlabel=Epoch, height=6cm, 
	xticklabel style={name=T\ticknum},grid=major]
	%C128, TODO: Redo with I12
	\addplot coordinates {
		(1,22.2)
		(2,21.6)
		(3,21.3)
		(4,21.1)
		(5,21.1)
		(6,21.0)
		(7,20.9)
		(8,21.1)		
		(9,21.3)
		(10,21.3)
		(11,21.2)
		(12,21.2)
		(13,21.2)
		(14,21.1)
		(15,21.1)
		(16,20.9)
		(17,20.8)
		(18,20.8)
	};
\addlegendentry{4 Layer};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Word Error Rate per Epoch when using newbob training. The exponential decaying of the learning rate started with epoch eight.}
	\label{fig:newbob_wer}
\end{minipage}
\\ \\
Figure \ref{fig:newbob_fer} shows the frame error rate for each epoch when training with newbob. It can be seen that exponential decay reduces the frame error rate significantly. \\ \\
\begin{minipage}{\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ylabel=Frame Error Rate, xlabel=Epoch, height=6cm, 
	xticklabel style={name=T\ticknum},grid=major]
	%I12
	\addplot coordinates {
		(1,57.54)
		(2,50.93)
		(3,49.71)
		(4,49.11)
		(5,45.57)
		(6,43.50)
		(7,42.30)
		(8,41.62)		
		(9,41.25)
		(10,41.06)
		(11,40.69)
		(12,40.91)
		(13,40.88)
		(14,40.87)
	};
	\addlegendentry{Train};
	\addplot coordinates {
		(1,52.61)
		(2,50.48)
		(3,49.70)
		(4,49.68)
		(5,46.57)
		(6,44.79)
		(7,43.89)
		(8,43.37)		
		(9,43.11)
		(10,42.99)
		(11,42.91)
		(12,42.89)
		(13,42.87)
		(14,42.87)
	};
	\addlegendentry{Test};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Frame Error Rate per Epoch when using newbob training. The exponential decaying of the learning rate started with epoch five.}
	\label{fig:newbob_fer}
\end{minipage}
\subsection{Sequence-based Training}
Since the experiments described in \cite{peddinti2015jhu} show improvement when sMBR sequence training is used, we attempted to use sequence based training as well. Our implementation used maximum mutual information estimation, as described in section \ref{sec:mmie}. We picked this training variant as a first step, since it is easier to implement than any variant of overall risk criterion estimation. Both approaches should show some improvement over maximum likelihood training on frame level, according to several bodies of work \cite{povey2005discriminative} \cite{ghoshal2013sequence}. \\ \\
For MMIE training, we pre-trained a four layer TDNN acoustic model with frame-based maximum likelihood for a single epoch. Then, we started MMIE training on a per-utterance basis. The training was done on multiple CPUs, the gradients were averaged before each stochastic gradient descend step. \\ 
While our experiments consistently showed high improvements in terms of frame error rate, the word error rate increased significantly: While the model reached a WER of $17.6$ using maximum likelihood estimation, a WER $25.6$ was reached after the MMIE training saturated. \\

\begin{minipage}{\linewidth}
	\begin{minipage}{0.5\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ylabel=Frame Error Rate, xlabel=Epoch, height=6cm, 
	ymin=50, ymax=70,
	xticklabel style={name=T\ticknum},grid=major]
	%1040
	\addplot coordinates {
		(1,66.47)
		(2,59.76)
		(3,58.47)
		(4,57.78)
		(5,57.12)
		(6,56.80)
		(7,56.63)
		(8,56.54)		
		(9,56.50)
		(10,56.47)
	};
	\addlegendentry{ML Train};
	\addplot coordinates {
		(1,65.43)
		(2,64.07)
		(3,63.49)
		(4,63.17)
		(5,62.98)
		(6,62.91)
		(7,62.90)
		(8,62.89)		
		(9,62.89)
		(10,62.89)
	};
	\addlegendentry{ML Test};
	\end{axis}
	\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.5\linewidth}
	\begin{tikzpicture}
	\begin{axis}[xlabel=\% of Epoch, height=6cm, 
	 ylabel near ticks, yticklabel pos=right,
	ymin=50, ymax=70,
	xticklabel style={name=T\ticknum},grid=major]
	%1040 SMBR 24
	\addplot coordinates {
		(10,59.64)
		(20,59.75)
		(30,58.51)
		(40,57.71)
		(50,56.68)
		(60,55.05)
		(70,55.46)
		(80,54.95)		
		(90,55.15)
		(100,54.66)
	};
	\addlegendentry{MMIE Test};
	\addplot coordinates {
		(10,59.68)
		(20,58.631)
		(30,57.96)
		(40,56.95)
		(50,55.49)
		(60,55.07)
		(70,54.92)
		(80,54.83)		
		(90,54.56)
		(100,54.35)
	};
	\addlegendentry{MMIE Train};
	\end{axis}
	\end{tikzpicture}
\end{minipage}
	\captionof{figure}{Frame Error Rate per Epoch when using maximum likelihood training, as well as Frame Error Rate over a single epoch}
	\label{fig:newbob_fer}
\end{minipage} \\ \\
Detailed results regarding the frame error rate are shown in figure \ref{fig:newbob_fer}. It can be seen that the MMIE training converged significantly faster. Also, the error on the test data set is significantly closer to the error on the training data set. \\ These results have to be taken with caution While the amount of data in both test data sets is the same, they are not equal as training and testing for MMIE happens on per-utterance basis, while the test and training sets for the maximum likelihood training were created on per-frame basis. \\ \\
Although the results regarding frame error rate look interesting, we did not pursue this approach any further, due to the resulting high word-error rates. 

\section{Decoding Parameters}
Tuning decoding parameters is important for good achieving a high accuracy on word level. The decoding parameters are not directly related to the acoustic model, but rather the decoding process. Different acoustic models might still require different decoding parameters for best performance. 
\subsection{Acoustic Model Scaling and Length Penalty}
Our experiments have shown that the $l_p$ and $l_z$ parameters are correlated. Therefore we have optimize them using a grid search over a reasonable parameter space. \\ \\
\begin{minipage}{\linewidth}
\begin{minipage}{0.5\linewidth}
\begin{tikzpicture}
\begin{axis}[grid=both,zlabel=Word Error Rate,xlabel=$l_z$,ylabel=$l_p$]
	\addplot3[surf,mesh/cols=6,z buffer=sort] file {data/lp_lz_996};
\end{axis}
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.5\linewidth}
\begin{tikzpicture}
\begin{axis}[grid=both,xlabel=$l_z$,ylabel=$l_p$]
\addplot3[surf,mesh/cols=5,z buffer=sort] file {data/lp_lz_128};
\end{axis}
\end{tikzpicture}
\end{minipage}
\captionof{figure}{Example of the word error rate for different $L_p$ and $L_z$ parameters for a four-layer TDNN (left) and a three-layer TDNN (right)}
\label{fig:lp_lz}
\end{minipage}
\\ \\
An example impact of the $L_p$ and $L_z$ for two different models are illustrated in figure \ref{fig:lp_lz}. Our experiments have shown that the optimal parameters are similar for each of the models we tested. It is still advisable to fine tune the parameters for each model. Taking the initial values for the grid search from an similar model usually leads good result.
\subsection{Master Beam}
We did tested several architectures with different master beams. Master beams between four and six appeared to work best, but we did not find any pattern that correlated with the network architecture. This indicates that the optimal master beam depends on several factors, not just on the architecture of the neural network model itself. 
\subsection{Neural Network Priors}
As described in section \ref{sec:ce_loss}, it is important to scale the posteriors generated by the neural network with priors. We tested two different approaches of generating the priors: Generating them from the complete test dataset and also generating them from the output of a trained model. In the second case, thirty minutes of speech data randomly from our dataset, computed the neural network output on it, and counted the occurrence of each label. \\ \\
\begin{minipage}{\linewidth}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	xbar,xmajorgrids=true,
	width=0.8\linewidth,height=3.5cm, enlarge y limits=0.5,
	xmin=14,xlabel={Word Error Rate},
	symbolic y coords={Dataset,Output},
	ytick=data,nodes near coords, nodes near coords align={horizontal},
	]
	\addplot coordinates {(15.4,Dataset) (15.1,Output)};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Word Error Rate for priors estimated from the dataset and the model output}
	\label{fig:wer_priors}
\end{minipage} 
\\
As illustrated in figure \ref{fig:wer_prior}, calculating the priors from the the output of the model decreased the word error rate significantly. 
\subsection{Softmax Smoothing}
We also tested the impact of softmax smoothing on the neural network output. The motivation is that a search through a beam search through a hidden markov model does not work very well when the acoustic model is overconfident regarding certain states. \\ \\
\begin{minipage}{\linewidth}
	\begin{tikzpicture}
	\begin{axis}[ylabel=Frame Error Rate, xlabel=Channels, height=6cm, 
	xticklabel style={name=T\ticknum},grid=major]
	%I12
	\addplot coordinates {
		(0.8,15)
		(0.85,15)
		(0.88,15)
		(0.9,14.9)
		(0.92,15)
		(0.95,15.1)
		(1,15.1)
		(1.1,15.4)
		(1.2,15.8)
	};
	\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Word Error Rate per softmax adjustment $1/\tau$ for a four-layer TDNN model}
	\label{fig:softmax_fer}
	
	TODO: If it makes a difference, show a augmented model here. 
\end{minipage} \\ \\


\iffalse
\label{ch:approach}
This chapter describes our approach for acoustic modelling using TDNNs on a high level. 
We will also give a brief overview over unknown hyperparemters and design decisions,
as well as a coarse overview over the structure of our implementation. 

\chapter{Experiment Setup}
\label{ch:experiment_setup}
This chapter should describe the detailed preleminaries and hyperparemters of our training and evaluation setup:
\begin{itemize}
    \item which data, and which preprocessing was used
    \item how was the data reverbed
    \item which learning rate schedulers, optimizers, loss functions were used
    \item which mechanisms were used to make the training faster and scalable
\end{itemize}
\fi