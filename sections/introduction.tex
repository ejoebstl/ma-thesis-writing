%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.2, 2017-08-01

\chapter{Introduction}
\label{ch:Introduction}

Automated Speech Recognition is an important way of human computer interaction. The fundamental problem Automated Speech Recognition attempts to solve is to transform natural spoken language to text, which can then easily be processed by computer systems. Especially with the advent of smart mobile devices, more and more use cases for Automated Speech Recognition are available, mainly in the form of smart assistants as described in \cite{lopez2017alexa}. There are also many use cases which are not facing end users, for example the automated transcription of university lectures, as outlined in \cite{muller2016lecture}.

Despite the recent success of automated speech recognition, many systems still rely on microphones which are close to the speaker, or microphone arrays and beam forming. For many use cases, this is a serious draw back. Users might want to use their smart assistant without picking up their device every time. During lectures, it is hard to transcribe questions from the audience. 

\cite{yoshioka2012making} gives a good overview about the problems that arise when distant microphones are used. The most prominent problem is reverberation. Reverberation happens, informally speaking, when a signal is interfered by weaker, delayed copies of itself. 

The goal of this work is to investigate how Automated Speech Recognition could be made more robust against reverberation. More specifically, a the focus is put on providing a more robust acoustic model using Time Delay Neural Networks. 

TODO: After we finished writing, give a short intro about each chapter. 

\chapter{Preliminaries}

This chapter summarizes concepts we built upon in this work. First, we give a brief introduction to signal processing and use this framework to explain the properties of reverberation in a more formal way. Then, we introduce hidden markov models, which are an important concept for understanding most speech recognition systems. 

\section{Reverberation and reverberated Audio}

This section will focus on a formal definition of reverberation and provide some intuition why reverberation makes Automated Speech Recognition difficult. First, we give a very brief introduction to signal processing and system theory, according to the book \cite{leon2015signale}.

\subsection{Continuous Signals and Systems}

Sound can, as any other continuous \textit{signal}, be described as a continous function $y(t)$ where $t$ indicates the time. In literature, the argument $t$ is often dropped  when not explicitly needed to make equations easier to read.  
Signals can be fed into \textit{systems}, which in turn creates an output signal. In the context of this work, we only consider linear, time invariant systems. 
\\
Let $S$ be a linear time invariant system, $y_1$, $y_2$ continuous signals, and $c_1$, $c_2$ constants. The following properties hold if and only if a given system is an linear time invariant system: 

\[S\{c_1 y_1(t) + c_2 y_2(t)\} = c_1 S\{y_1(t)\} + c_2 S\{y_2(t)\} \tag{linearity}\]

The linear property allows us to treat application of a system to a signal as a linear transformation in the function space our signals are defined in. 

\[y_1(t) = S\{y_2(t)\} \implies y_1(t - t0) = S\{y_2(t - t0)\} \tag{time invariance}\]

The time invariance property guarantees that the behavior of a system never depends on the time. In other words, if the input signal to a system is shifted in time, the only difference to the output is a shift in time as well.

\[
\begin{rcases*}
y_1(t') = y_2(t') \\
x_1(t') = S\{y_1(t')\} \\ 
x_2(t') = S\{y_2(t')\} \\
\end{rcases*} \implies x_1(t') = x_2(t') \tag{causality}\]

The causality property guarantees that, if two signals are equal for all times $t'$ before a chosen time $t_0$, the output signals of the system processing this signals will also be equal up to this point. Simply put, the output of a system up to time $t_0$ can not depend on any input that happens after $t_0$. It is worth to note that all real systems are always causal. 
\\ \\
A linear time-invariant system can be characterized by its so called impulse response $g$, defined as the system output when presented with a so called dirac impulse $\delta$.

\[g(t) = S\{\delta(t)\}\]

The dirac impulse $\delta$ is a function that is formally defined by the following equation. 

\begin{align}
g(t_2) = \int_{-\infty}^{\infty} y(t)\delta(t - t_2) dt \label{eq:dirac} 
\end{align}

It can be said that the dirac impulse is zero for all $t$ not equal to zero. The integral over the dirac impulse defined to be one. 
\\ \\
Given definition \ref{eq:dirac}, as well as the linear property, we can show that the impulse response of a system is indeed sufficient to calculate the output signal for any given input signal. 
\begin{align*}
x(t) &= S\{y(t)\} \\
	 &= S\left\{\int_{-\infty}^{\infty} y(\tau)\delta(\tau - t) d\tau \right\} \\
	 &= \int_{-\infty}^{\infty} y(\tau)S\{\delta(\tau - t)\} d\tau \\
	 &= \int_{-\infty}^{\infty} y(\tau)g(\tau - t) d\tau
\end{align*}

Furthermore, we define the convolution operation, $*$, for two given signals as follows.

\[
 (y_1 * y_2)(t) = \int_{-\infty}^{\infty} y(\tau)y_2(\tau - t) d\tau \tag{convolution}
\]

A convolution is thus an operation that combines two functions to create a new function. Given this definition, we can write the output signal $x$ of a system $S$ given an input signal $y$ as convolution with the impulse response $g$ of the system. 

\[
x(t) = S\{y(t)\} = (y * g)(t)
\]

\subsection{Properties of Reverberation}

The acoustic properties of a room can be approximated as a linear time invariant system. The properties of this system are dependent on the properties of the room itself, for example the shape, size and surface of the wall, as well as the location of the sound source and the location of the receiver. Especially regarding automated speech recognition, \cite{ritter2016training} gives results that show that the location of a speaker relative to the microphone can have a very large impact on recognition results.

The measured impulse response of a real reverberation can be seen in figure \ref{fig:air_rir}. This specific sample is taken from the Aachen Impulse Response database \cite{jeub2009binaural}. Such a sample can be created by creating a very brief sound impulse, for example a clap, in an otherwise silent room, and then recording the sound for a few seconds. 

As described in \cite{yoshioka2012making}, we can divide the impulse response into the direct sound itself, early reflections, and late reverberation. The intuition behind is that the original sound wave arrives at the receiver first. After that, reflections of the signal which were reflected once by the walls of the room arrive. These reflections are already dampened significantly. Then, reflections of reflections will be received, and so on, until the sound waves become so weak that the room is silent again. 
\\ \\
\begin{minipage}{\linewidth}
	\makebox[\linewidth]{
		\begin{tikzpicture}
			\begin{axis}[ytick=\empty,height=6cm, ylabel=Amplitude, xlabel=Milliseconds, 
			xticklabel style={name=T\ticknum}]
			\addplot table [x expr=\coordindex / 4,y=amplitude,color=black,mark=none] {data/air0053bilecture};
			
			\end{axis}	
			\draw[] (1.2,4) -- (1,4) node[anchor=east,font=\tiny] {Sound};
			\draw[decorate, decoration={brace}] (1.3,3.2) -- (3.00,3.2) node[midway, anchor=south,font=\tiny] {Early Reflections};	
			\draw[decorate, decoration={brace}] (3.05,3.2) -- (11.8,3.2) node[midway, anchor=south,font=\tiny] {Late Reverberations};
		\end{tikzpicture}
	}	
	\captionof{figure}{Room impulse response of a lecture hall}
	\label{fig:air_rir}
\end{minipage}

It is important to note that late reverberations can be measurable for several hundred milliseconds.\\\\

To illustrate the impact of reflection and reverberation in a more formal way, we can use the decomposition into direct sound, early reflection, and late reverberation. We define a rather crude approximation of a room impulse response that subsequently overlays an audio signal with weaker copies of itself.

\[
g_{rir}(t) = w_0 \delta(t) + \sum_{1}^{n} w_n * \delta(t - t_n)
\]

Here, $w_n$ are weighting factors, which represent the dampening of our reflections. $t_n$ are the delays until our reflection is received. We now consider the system $S_{rir}$ associated with the impulse response $g_{rir}$, apply the audio signal $y$ and observe the output $x$.

\begin{align*}
x(t) &= S_{rir}\{y(t)\} \\
     &= (g_{rir} * y)(t) \\
     &= \int_{-\infty}^{\infty} g(\tau)y(\tau - t) d\tau \\
     &= \int_{-\infty}^{\infty} \left[ w_0 \delta(t) + \sum_{n} w_n * \delta(t - t_n)\right] y(\tau - t) d\tau \\
     &= \int_{-\infty}^{\infty} \delta(t) y(\tau - t) d\tau + \sum_{n}  \int_{-\infty}^{\infty} w_n \delta(t - t_n) y(\tau - t) d\tau  \\
     &= w_0 y(t) + \sum_{n} w_n y(t - t_n)
\end{align*}

If the room impulse response is non-zero over at a certain time interval $t_n$, the audio signal produced at $t$ will still influence the received signal $x$ at $t + t_n$. 

\subsection{Impact of Reverberation on Digital Audio Samples}

Before formally introducing automated speech recognition during a later chapter, we want to show that the impact of reverberation can be significant for many applications that process sound or speech.\\ \\
Before a signal can be processed on a computer, it has to be measured. This process is called \textit{sampling}. Formally, we can describe sampling as the following operation, where $t_A$ is called the sampling interval. We call $y_{digital}$ a discrete signal. 

\[
y_{digital}(t) = y(t) * \sum_{n = 0}^{\infty} \delta{t - nt_A} 
\]

Since this yields a time series of infinite length, signals are usually cut into pieces, which are then independently processed from each other. This is called windowing. For the most simple form of windowing, we can set all signal values outside of a certain range to be zero. Formally, this can be defined by multiplying the signal with a rectangle function $\sigma_{rect,a}(t)$ .

\[
\sigma_{rect,a}(t) = \begin{cases}
1 &,-a < t < a \\
0
\end{cases}
\]

When working with signals, especially for classification tasks, methods built upon the \textit{fourier transform} are used very often. The fourier transform transforms a signal $y(t)$ from its time domain to the frequency domain $Y(f)$. The resulting function $Y(f)$ is called the spectrum and gives the distribution of energy over all frequencies for the original signal $y(t)$. The fourier transformation can be defined for continuous or discrete signals, as well as for discrete and windowed signals. In the case of discrete and windowed signal, this is called the \textit{short time fourier transform}, which was first described in \cite{gabor1946theory}. It can formally be defined as follows, with an arbitrary window function $\sigma$:

\begin{align}
Y(n, \omega) = \sum_{m = -\infty}^{\infty} y(mt_A) \sigma((n - m)t_A) e^{-j \omega n}  
\label{eq:stft}
\end{align}

The function $Y(n, \omega)$ gives the signal magnitude for a certain time window $n$ and a frequency window $\omega$. The time resolution depends reciprocally on the frequency resolution and vice versa. It is not possible to increase the frequency resolution while not decreasing the time resolution.\\ \\

We can investigate the effects of a reverberated signal on the short time fourier transform by applying the same approach as in the previous chapter. The resulting short time fourier spectrum of the reverberated and sampled signal is given as follows.

\begin{align}
Y(n, \omega) = \sum_{m = -\infty}^{\infty} w_0 y(mt_A) \sigma((n - m)t_A) e^{-j \omega n} + 
\sum_{m = -\infty}^{\infty} \sum_{n} w_n y(mt_A - t_n) \sigma((n - m)t_A) e^{-j \omega n}  
\label{eq:stftnoise}
\end{align}

Equation \ref{eq:stftnoise} shows that, for sufficiently large $t_n$ and $w_n$, significant noise is added to neighboring time windows. To recapitulate from the last chapter, the $t_n$ for a large room can be up to several tenths of seconds, while the window size for most applications is in the hundreds of milliseconds. 
\\ \\
This mathematic observation shall serve as a motivation for this work. Reverberation, especially late reverberation can be a hard problem that significantly distorts measurements of signals. To cope with reverberation, our application has to consider large time windows in the first place. Time delay neural networks, which will be introduced in section \ref{ch:TDNN} can naturally deal with a such wide windows in a stable way. Before we explain the properties of time delay neural networks in depth, we first introduce neural networks in section \ref{sec:neural_networks} as well as the basics of automated speech recognition in section \ref{ch:HMM_ASR}. In chapter \ref{ch:approach} we will explain our solution in detail and then show experimental results in chapter \ref{ch:results}.
\\ \\
We conclude the introduction with a more visual example. Figure \ref{fig:air_spectrogram} shows the magnitude of the short term fourier transformation for a short speech segment, as well as the short term fourier transformation for the same speech segment after it has been reverberated using the impulse response shown in Figure \ref{fig:air_rir}. Such a magnitude representation is also called \textit{spectrogram} in literature. 


\begin{minipage}{\linewidth}
	\makebox[\linewidth]{
		\begin{tikzpicture}
		\begin{axis}[xlabel=Time, ylabel=Frequency, ytick=\empty, width= .50\textwidth, axis line style={draw=none}, axis x line*=bottom]
		\addplot [] 
		graphics[xmin=0,xmax=3,ymin=0,ymax=1] {images/abc_spectrogram.png};
		\end{axis}
		\end{tikzpicture}
		\begin{tikzpicture}
		\begin{axis}[xlabel=Time, ylabel=\empty, ytick=\empty, width= .50\textwidth, axis line style={draw=none}, axis x line*=bottom]
		\addplot [] 
		graphics[xmin=0,xmax=3,ymin=0,ymax=1] {images/abc_noise_spectrogram.png};
		\end{axis}
		\end{tikzpicture}
	}
	\captionof{figure}{Spectrogram of a clean and a reverberated audio sample. The left side shows a clean recording of a speaker saying \textit{``A B C''}. The right side shows the the recording after it was reverberated by the impulse response shown in Figure \ref{fig:air_rir}. It can clearly be seen that the reverberation caused the signal to become smudged along the time axis.}
	\label{fig:air_spectrogram}
\end{minipage}


\section{Hidden Markov Models}
To explain hidden markov models, we first introduce the concept of a \textit{markov chain}. A markov chain is a sequence of random variables $X = x_1, x_2, \dots. x_{t - 1}, x_{t} \dots, x_T$ and a finite number of states $s_1, \dots, s_n$, where the probability of each entering a certain state at time $t + 1$ at only depends on the state at time $t$:

\[
P(x_{t + 1} = s_{j_{t + 1}} | x_{t} = s_{j_t}, x_{t - 1} = s_{j_{t - 1}} \dots, x_{1} = s_{j_{1}}) = P(x_{t + 1} = s_{j_{t + 1}} | x_{t} = s_{j_t})
\] 

This assumption is also called the \textit{markov assumption}. If the probability of moving from state $s_{j_t}$ to state $s_{j_{t + 1}}$ is independent of the current time $t$, we call a markov chain \textit{homogeneous}. \\ \\
We now extend our homogeneous markov chain and assume that we can no longer observe the state $s_{j_t}$ at a given time $t$ directly, but a symbol $v_k$ that was emitted. We formally define: 
\begin{align*}
S &= \{s_1, \dots, s_n\} \tag{states} \\
V &= \{v_1, \dots, v_m\} \tag{symbols} \\
A &= (a_{ij}) \tag{state tansmission probability} \\
a_{ij} &= p(x_{t+1} = s_j | x_{t} = s_i) \\
B(k) &= (b_j(k)) \tag{emisson probability} \\
b_j(k) &= p(v_k | x_t = s_j) \\
\pi &= (\pi_i) \tag{initial state probability} \\
\pi_i &= p(x_1 = s_i)
\end{align*}

The tuple $\lambda = (S, V, A, B, \pi)$ specifies a \textit{hidden markov model}. We furthermore introduce the graphical notation for hidden markov models seen in figure \ref{fig:hmm}. Unspecified transitions and observations have probability zero. 

\begin{minipage}{\linewidth}
	\makebox[\linewidth]{
		\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
		
		\node[state] (s1) at (0,2) {$s_1$}
		edge [loop above] node[above] {$a_{11}$} ();
		\node[state] (s2) at (3,2) {$s_2$}
		edge [<-,bend right=45] node[auto,swap] {$a_{12}$} (s1)
		edge [->,bend left=45] node[auto,swap] {$a_{21}$} (s1)
		edge [loop above] node[above] {$a_{22}$} ();
		\node[state] (s3) at (6,2) {$s_3$}
		edge [<-,bend right=45] node[auto,swap] {$a_{23}$} (s2)
		edge [->,bend left=45] node[auto,swap] {$a_{32}$} (s2)
		edge [loop above] node[above] {$a_{33}$} ();
		% observations
		\node[observation] (v1) at (1.5,0) {$v_1$}
		edge [lightedge] node[auto] {$b_1(1)$} (s1)
		edge [lightedge] node[auto,swap] {$b_2(1)$} (s2);
		\node[observation] (v2) at (4.5,0) {$v_2$}
		edge [lightedge] node[auto] {$b_2(2)$} (s2)
		edge [lightedge] node[auto,swap] {$b_3(2)$} (s3);
		\end{tikzpicture}
	}
	\captionof{figure}{Graphical example of a hidden markov model with three states and two observations}
	\label{fig:hmm}
	\hspace{1cm}
\end{minipage}

\subsection{Forward and Backward algorithm}
Given a sequence of observed emissions, a so called observation $O = \{o_1, \dots, o_t\}$, a hidden markov model can be used to evaluate the probability of the observation, given the parameters $p(O, \lambda)$. For calculating this joint probability, the \textit{forward algorithm} or the \textit{backward algorithm} can be used used. For the forward algorithm, let $\alpha_T(j)$ be the probability of having observed $O$ and being in state $s_j$ at the end of the observation. We define recursively for any previous time $t$:
\begin{align*}
\alpha_1(j) &= \pi_j b_j(o_1) \\
\alpha_t(j) &= b_j \sum_{i = 1}^{N} a_{ij}\alpha_{t-1}(i) \\
p(O|\lambda) &= \sum_{j = 1}^{N} \alpha_T(j)
\end{align*}
For the backward algorithm, we define a similar recursive algorithm, starting from the latest time observation. Here, $\beta_0(j)$ is the probability of observing $O$ when starting from state $s_j$.
\begin{align*}
\beta_T(j) &= 1 \\
\beta_t(j) &= \sum_{i = 1}^{N} a_{ij}b_j(o_{t+1}) \beta_{t+1}(i) \\
p(O|\lambda) &= \sum_{j = 1}^{N} \beta_0(j)
\end{align*}
\subsection{Decoding Problem}
Besides calculating the probability of an observation sequence, finding the most likely state sequence $X = \{X_1, X_2, \dots, X_T\}$ given an observation $O$ and a hidden markov model $\lambda$ is also of interest. 
This problem is called the \textit{decoding problem}, which is solved by the \textit{viterbi algorithm}. In the scope of this work, a formal definition of the viterbi algorithm is not necessary. 

TODO: Explain viterbi. 

\subsection{Learning Hidden Markov Model Parameters}
\label{sec:learning_hmm}
Given a hidden markov model topology, which is essentially only the number of states $n$ and the number of emission symbols $m$, as well as a training set of observations $O$, we can use the so called \textit{Baum-Welch} algorithm to optimize the initial state probabilities $\pi$, transmission probabilities $A$ and emission probabilities $B$. \\ \\
We first define two auxiliary variables, given an observation $O$ and the parameters $\lambda$: $\gamma_t(i)$, the probability of being in state $i$ at time $t$ and $\xi_t(i, j)$, the probability of being $i$ at time $t$ and state $j$ at time $t + 1$.

\begin{minipage}{0.45\textwidth}
	\begin{align*}
	\gamma_t(i) &= P(x_t = i|O,\lambda) \\
	&= \frac{P(x_t = i, O|\lambda)}{P(O|\lambda)} \\
	&=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j = 1}^{N} \alpha_t(j) \beta_t(j)} 
	\end{align*}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
	\begin{align*}
	\xi_t(i, j) &= P(x_t = i,x_{t + 1} = j|O,\lambda) \\
	&= \frac{P(x_t = i,x_{t + 1} = j, O|\lambda)}{P(O|\lambda)} \\
	&=\frac{\alpha_t(i)a_{ij}\beta_{t + 1}(j)b_j(o_{t + 1})}{\sum_{i = 1}^{N} \sum_{j = 1}^{N} \alpha_t(i)a_{ij}\beta_{t + 1}(j)b_j(o_{t + 1})}
	\end{align*}
\end{minipage}

Using this two auxiliary variables, we can find new parameters $\lambda$ iteratively. Let $\pi^*$, $A^*$ and $B^*$ be the new parameters after one iteration of the Baum-Welch algorithm. 

\begin{align*}
b^*_j(k) &= \frac{\sum_{t = 1, o_t = v_k}^{T} \gamma_t(j)}{\sum_{t = 1}^{T} \gamma_t(j)} \\
\pi^*_i &= \gamma_1(i) \\
a_{ij}^*  &= \frac{\sum_{t = 1}^{T} \xi_t(i, j)}{\sum_{t = 1}^{T} \gamma_t(j)} 
\end{align*}

It should be noted that the Baum-Welch algorithm is a special case of the \textit{expected maximization} {\textit{EM}} algorithm applied to hidden markov models. The expected maximization algorithm gives a maximum-likelihood estimate of parameters, even if the data set used for the estimation has incomplete or missing values. A proof can be found in \cite{bilmes1998gentle}. 