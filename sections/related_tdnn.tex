
\section{Time Delay Neural Networks}
\label{sec:tdnn}

\textit{Time delay neural networks} were introduced by Waibel et\ al. in \cite{waibel1990phoneme}. The purpose of time delay neural networks is to model long time dependencies in a robust way. The central idea behind this network type is to use several time frames of input as well as layers that impose temporal structure. \\ \\
To achieve this, we not only consider the current frame $x_{0,t}$, but also the $n$ previous frames $x_{0,t - 1}, ..., x_{0,t - n}$ as input. These frames are delayed in time, hence the name. Then, the a weighted sum is applied to the input, where the weights are learned parameters. This is called a \textit{TDNN unit} or \textit{TDNN layer} with a filter kernel of size $n$. TDNN layers can be stacked. If this is the case, preceding layers have to be extended to produce an output that contains multiple time frames. We call a network of stacked TDNN layers time delay neural network. \\
Formally, we can calculate the output of the $k$-th TDNN layer at time $t$ for feature vectors with size one as follows:
\[
x_{k,t} = \sum_{i = 1}^n w_{k, i} x_{k - 1, t - i}
\]
Where $w_{k, i}$ is denotes the $i$-th weight of the weighted sum used by the $k$-th layer and $x_{k,t}$ denotes the output of the $k$-th layer at time $t$. \\
This equation can be rewritten using a discrete convolution operator. In this case $w_k$ is called a convolution kernel. 
\[
x_{k} = w_{k} * x_{k - 1}
\]
For feature vectors containing more than one feature, we introduce the concept of \textit{channels}. A channel can be considered a dimension in the feature. We can write this case for $p$ input channels and $q$ output channels, where $w_{k, j}$ is now a multi-dimensional kernel, producing the output channel $j$. Each TDNN layer will learn $q$ such kernels, while each kernel is large enough to take all $p$ input channels into account. 
\[
x_{k,j} = w_{k, j} * x_{k - 1}
\]
The concept of interpreting TDNN layers as convolutions is not new. The generalization to multi-dimensional convolutions is called \textit{convolutional neural networks} and was shown to be incredibly successful \cite{krizhevsky2012imagenet}. Since TDNNs only apply the convolution over the time dimension, it can be useful to interpret a TDNN as a so called \textit{finite impulse response} (\textit{FIR}) filter. As described in \cite{leon2015signale}, finite impulse response filters are inherently stable, as they the output is always a sum of finite elements. They also do not accumulate rounding errors. \\
In \cite{waibel1990phoneme}, another interesting property is given: Since there are a lot less learned parameters than operations, a TDNN layer is forced to only focus on the most important features in the data, which leads to better generalization. This so-called \textit{parameter sharing} is achieved by averaging gradients of all operations for the respective weight, independent of the time context $t$. According to \cite{Goodfellow-et-al-2016}, this is one of the main reasons of success for convolutional neural networks in general. \\ \\
It should be noted that in modern TDNNs, the input context of a layer often extends to future time frames. An example would be the input vector $x_{0,t + n},...,x_{0,t},..., x_{0,t - n}$.

\subsection{Pooling, Stride and Splicing}

Many successful convolutional architectures, as \cite{krizhevsky2012imagenet}, combine convolutional layers with pooling nonlinearities. This is done because pooling over the layer output enables the network to learn several different representations of the same concept, where the pooling will forward the output of the most dominant representation to the next layer  \cite{Goodfellow-et-al-2016}. \\
Furthermore, it was shown that using larger steps in the convolution operation can lead to better results \cite{springenberg2014striving}. In the context of TDNNs, this means that we would not use adjacent frames for concatenating our input, but frames which are further apart. In literature, this is called \textit{stride} or \textit{strided convolution}, where the stride $s$ gives the distance between concatenated frames. For a strided convolution, the input vector can be written as:
\[
x_{0,t}, x_{0,t + s}, ..., x_{0,t + ns}
\] 
It is also possible to define a list of indexes $S = (s_1, ..., s_n)$ to concatenate, which is especially useful if the distance between concatenated frames should not be uniform. This operation is called \textit{splicing} and was introduced by \cite{peddinti2015jhu}. In this case, our input vector becomes: 
\[
x_{0,ts_1}, x_{0,t + s_2}, ..., x_{0,t + s_n}
\] 

\subsection{A Visual Example}

TDNNs allow a straight-forward interpretation of their layer outputs as multi-dimensional signals, which makes visualization over time helpful. We want to conclude this section with the visualization of a tiny TDNN that was trained to distinguish $53$ phones in an audio sample. We use $40$ log-mel coefficients as features. Log-mel coefficients are introduced in section \ref{sec:lmel}. \\ \\
Figure \ref{fig:tiny_tdnn} shows a tiny TDNN model in the time domain. Our input vector has $40$ channels and a time context of $21$. The first TDNN layer uses $40$ filter kernels of size $5*40$ and a stride of $4$. It and is followed by an L2 polling with group size $2$, resulting in a layer output with $5$ time frames and $20$ channels. The second TDNN layer uses $40$ filter kernels of size $5*20$ and a stride of $5$. It is also followed by an L2 pooling with group size $2$, resulting in an output spanning $1$ time frame and $20$ channels. In the end, we apply a linear layer that maps the $20$ channels to $53$ phone classes, including special classes for silence and noise.\\
\begin{minipage}{\linewidth}
	\vspace{5mm}
	\begin{tikzpicture}[x=1.8cm, y=1.5cm]
	% Layers
	\node[text width=3cm] at (-1,0) {Input ($21\times40$)};
	\foreach \m in {2,3,...,20}
	\node [tdnn neuron] (input-\m) at (\m*0.25,0) {};
	
	\node [tdnn neuron, label=below:$x_{t - 10}$] (input-1) at (1*0.25,0) {};
	\node [tdnn neuron, label=below:$x_{t + 10}$] (input-21) at (21*0.25,0) {};
	
	\node[text width=3cm] at (-1,0.5) {TDNN 1/L2 pool};
	\node[text width=3cm] at (-1,1) {Hidden ($5\times20$)};
	\foreach \m [count=\y] in {1,2,...,5}
	\node [tdnn neuron] (hidden-1-\m) at (\y*0.25*4 - 0.25,1) {};
	
	\node[text width=3cm] at (-1,1.5) {TDNN 2/L2 pool};
	\node[text width=3cm] at (-1,2) {Output ($1\times20$)};
	\node [tdnn neuron, label=above:$y_{t}$] (classify-1) at (1*0.25 + 10 * 0.25,2) {};
	
	%Edges
	\foreach \m [
	evaluate=\m as \nstart using int(((\m - 1) * 4) + 1),
	evaluate=\m as \nstep using int(((\m - 1) * 4) + 2),
	evaluate=\m as \nend using int(((\m - 1)* 4) + 5)] in {1,2,...,5}
	\foreach \i in {\nstart,\nstep,...,\nend}
	\draw (input-\i.north) -- (hidden-1-\m); % Error happens here. 
	
	\foreach \m in {1,2,...,5}
	\draw (hidden-1-\m) -- (classify-1);
		
	\end{tikzpicture}
	\captionof{figure}{Tiny TDNN model}
	\label{fig:tiny_tdnn}
	\vspace{5mm}
\end{minipage}

We trained this tiny TDNN model on $14$ hours of voice data using SGD and the newbob-learning rate scheduler. Figures \ref{fig:tiny_tdnn_1_weights}, \ref{fig:tiny_tdnn_2_weights} and \ref{fig:tiny_classifier_weights} show the learned filter kernels and weights after training. Brighter colors indicate higher values, the horizontal axis corresponds to time, while the vertical axis corresponds to channels. For the first TDNN layer, it can be seen that filters which are in the same pooling group learn similar parameters. For the other layers, the filters are harder to implement, as TDNN layers do not preserve the order of channels. They are still shown for completeness. \\
\noindent
\weightillustration
	{images/tdnn_example/large_weight_tdnn_1.png}
	{$40$ learned filters of TDNN layer $1$}
	{fig:tiny_tdnn_1_weights}
\weightillustration
	{images/tdnn_example/large_weight_tdnn_2.png}
	{$40$ learned filters of TDNN layer $2$}
	{fig:tiny_tdnn_2_weights}
\smallweightillustration
	{images/tdnn_example/large_weight_classifier.png}
	{Affine transformation learned by the final linear layer}
	{fig:tiny_classifier_weights}

We now visualize the outputs of different layers given an audio sample of a few seconds length. Figures \ref{fig:tiny_input_signal} to \ref{fig:tiny_softmax_out} show the output of the TDNN and L2 pooling layers, as well as the output after the linear and the softmax layer. Such layer outputs are also called \textit{activations}. The input features, 40 log-mel features per time frame, are shown in Figure \ref{fig:tiny_input_signal} and originate from a female speaker saying \textit{``And these are always periods, ladies and gentlemen, accompanied by turbulence.''}.

\noindent
\weightillustration
	{images/tdnn_example/large_0_input.png}
	{Input features extracted from the audio sample}
	{fig:tiny_input_signal}
\weightillustration
	{images/tdnn_example/large_1_tdnn_1.png}
	{Output of the first TDNN layer}
	{fig:tiny_tdnn_1_out}
\weightillustration
	{images/tdnn_example/large_4_lp_1.png}
	{Output of the first L2 pooling layer}
	{fig:tiny_lp_1_out}
\weightillustration
	{images/tdnn_example/large_5_tdnn_2.png}
	{Output of the second TDNN layer}
	{fig:tiny_tdnn_2_out}
\weightillustration
	{images/tdnn_example/large_8_lp_2.png}
	{Output of the second L2 pooling layer}
	{fig:tiny_lp_2_out}
\weightillustration
	{images/tdnn_example/large_10_classifier.png}
	{Output of the linear layer}
	{fig:tiny_classifier_out}
\weightillustration
	{images/tdnn_example/large_11_softmax.png}
	{Output of the softmax layer, the final network output}
	{fig:tiny_softmax_out}

We observe the TDNN outputs in figures \ref{fig:tiny_tdnn_1_out} and \ref{fig:tiny_tdnn_2_out} and their respective L2 pooling outputs in figures \ref{fig:tiny_lp_1_out} and \ref{fig:tiny_lp_2_out}. It can be seen that the TDNN filter kernels generated a signal with multiple channels, where the L2 pooling reduced the number of channels but preserved dominant features. The maximum activations in the TDNN layer output are still dominant after the L2 pooling. We can see that the linear layer output in figure \ref{fig:tiny_classifier_out} generated a prediction for each time frame by combining the features generated by the previous TDNN and L2 pooling layers. Here, the maximum activation corresponds to the most likely phone. The softmax layer in figure \ref{fig:tiny_softmax_out} essentially increases contrast, so the phone with the maximum probability can be easily seen. In this case, each row in the image corresponds to a single phone class, whereas each column corresponds to a time frame. 

%24: PaddyAshdown_2011X\PaddyAshdown_2011X_71_6: and these are always periods ladies and gentlemen accompanied by turbulence
